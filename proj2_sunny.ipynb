{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit. These are the only imports permitted.\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.tree import DecisionTreeClassifier   # for Task 4\n",
    "from sklearn.base import clone                    # optional for Task 4\n",
    "import matplotlib.pyplot as plt                   # for Task 5\n",
    "from sklearn.metrics.pairwise import rbf_kernel   # for Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB(ABC):\n",
    "    \"\"\"Base class for a contextual multi-armed bandit (MAB)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms):\n",
    "        if not np.issubdtype(type(n_arms), np.integer):\n",
    "            raise TypeError(\"`n_arms` must be an integer\")\n",
    "        if not n_arms >= 0:\n",
    "            raise ValueError(\"`n_arms` must be non-negative\")\n",
    "        self.n_arms = n_arms\n",
    "        # your code here (if you like)\n",
    "        \n",
    "    @abstractmethod\n",
    "    def play(self, pround,  context):\n",
    "        \"\"\"Play a round\n",
    "        \n",
    "        Parameters\n",
    "        ----------     \n",
    "        pround : play round\n",
    "           \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors presented to the MAB. The 0-th \n",
    "            axis indexes the arms, and the 1-st axis indexes the features.\n",
    "            Non-contextual bandits accept a context of None.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        arm : int\n",
    "            Integer index of the arm played this round. Should be in the set \n",
    "            {0, ..., n_arms - 1}.\n",
    "        \"\"\"\n",
    "        # your code here (if you like)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self, arm, reward, context):\n",
    "        \"\"\"Update the internal state of the MAB after a play\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arm : int\n",
    "            Integer index of the played arm in the set {0, ..., n_arms - 1}.\n",
    "        \n",
    "        reward : float\n",
    "            Reward received from the arm.\n",
    "        \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors that was presented to the MAB. The \n",
    "            0-th axis indexes the arms, and the 1-st axis indexes the \n",
    "            features. Non-contextual bandits accept a context of None. \n",
    "        \"\"\"\n",
    "        # your code here (if you like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global functions here, if required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement Îµ-greedy and UCB MABs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsGreedy(MAB):\n",
    "    \"\"\"Epsilon-Greedy multi-armed bandit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms\n",
    "\n",
    "    epsilon : float\n",
    "        Explore probability. Must be in the interval [0, 1].\n",
    "\n",
    "    Q0 : float, default=np.inf\n",
    "        Initial value for the arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, epsilon, Q0=np.inf):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "        self.epsilon = epsilon\n",
    "        self.n_arms = n_arms  #Number of arms\n",
    "        self.Q = np.full(n_arms, Q0)  #Reward : Init w/ Q0\n",
    "        self.pullCnt = np.zeros(n_arms)  #for each arm\n",
    "        self.totRewards = np.zeros(n_arms)  #reward from each arm\n",
    "        \n",
    "    def play(self, pround, context=None):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "        # Exploitation - pick the best expectation\n",
    "        if np.random.random() > self.epsilon:\n",
    "            best_arms = np.argwhere(self.Q == np.amax(self.Q)).flatten().tolist()\n",
    "            arm_played = np.random.choice(best_arms) #tie case\n",
    "        \n",
    "        # Exploration - pick random\n",
    "        else:\n",
    "            arm_played = np.random.randint(self.n_arms)  #randome int among arms\n",
    "        \n",
    "        return arm_played + 1\n",
    "        \n",
    "        \n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here\n",
    "        idx_arm = arm - 1\n",
    "        #cal Q\n",
    "        self.totRewards[idx_arm] += reward\n",
    "        self.pullCnt[idx_arm] += 1\n",
    "        cnt = self.pullCnt[idx_arm]\n",
    "        \n",
    "        if (cnt>0) : self.Q[idx_arm] = self.totRewards[idx_arm] / cnt\n",
    "        else : print(\"update : cnt <= 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(MAB):\n",
    "    \"\"\"Upper Confidence Bound (UCB) multi-armed bandit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    rho : float\n",
    "        Positive real explore-exploit parameter.\n",
    "\n",
    "    Q0 : float, default=np.inf\n",
    "        Initial value for the arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, rho, Q0=np.inf):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "        self.rho = rho  # Exploration hyper parameter\n",
    "        self.Mu = np.zeros(n_arms)  # avg reward of each arm\n",
    "        \n",
    "        self.n_arms = n_arms\n",
    "        self.Q = np.full(n_arms, Q0)\n",
    "        self.pullCnt = np.zeros(n_arms)\n",
    "        self.totRewards = np.zeros(n_arms)\n",
    "    \n",
    "    def play(self, pround, context=None):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "        # Update expected reward Q for each arm.\n",
    "        for idx_arm in range(self.n_arms):            \n",
    "            cnt = self.pullCnt[idx_arm]\n",
    "            if cnt > 0:  \n",
    "                self.Q[idx_arm] = self.Mu[idx_arm] + np.sqrt(self.rho * np.log(pround) / cnt)\n",
    "        #Pick the best arm w/ highest Q\n",
    "        best_arms = np.argwhere(self.Q == np.amax(self.Q)).flatten().tolist()\n",
    "        arm_played = np.random.choice(best_arms)\n",
    "        return arm_played + 1\n",
    "        \n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here\n",
    "        \n",
    "        # Update Mu w/ avg(observed reward)\n",
    "        idx_arm = arm - 1\n",
    "        self.totRewards[idx_arm] += reward\n",
    "        self.pullCnt[idx_arm] += 1\n",
    "        \n",
    "        cnt = self.pullCnt[idx_arm]\n",
    "        rwds = self.totRewards[idx_arm]\n",
    "\n",
    "        if cnt > 0:        \n",
    "            self.Mu[idx_arm] = rwds / cnt\n",
    "        else : print(\"update : cnt <= 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement off-policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def offlineEvaluate(mab, arms, rewards, contexts, n_rounds=None):\n",
    "    \"\"\"Offline evaluation of a multi-armed bandit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mab : instance of MAB\n",
    "        MAB to evaluate.\n",
    "    \n",
    "    arms : integer numpy.ndarray, shape (n_events,) \n",
    "        Array containing the history of pulled arms, represented as integer \n",
    "        indices in the set {0, ..., mab.n_arms}\n",
    "    \n",
    "    rewards : float numpy.ndarray, shape (n_events,)\n",
    "        Array containing the history of rewards.\n",
    "    \n",
    "    contexts : float numpy.ndarray, shape (n_events, n_arms, n_dims)\n",
    "        Array containing the history of contexts presented to the arms. \n",
    "        The 0-th axis indexes the events in the history, the 1-st axis \n",
    "        indexes the arms and the 2-nd axis indexed the features.\n",
    "        \n",
    "    n_rounds : int, default=None\n",
    "        Number of matching events to evaluate the MAB on. If None, \n",
    "        continue evaluating until the historical events are exhausted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : float numpy.ndarray\n",
    "        Rewards for the matching events.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    tround = 0\n",
    "    R = []  # initial payoffs is empty\n",
    "    # Note: history object as mentioned in the original algorithm is not maintained as an object here\n",
    "    #       as it is not used in our implementation\n",
    "    \n",
    "    # If nrounds is not set, will go through all logs from the given datasets\n",
    "    if n_rounds is None:\n",
    "        n_rounds = len(dataset)\n",
    "    \n",
    "    for log in range(len(dataset)):  #step through the stream of logged events one by one\n",
    "        \n",
    "        # Once function finds (nrounds) matching arm plays, it should stop\n",
    "        if tround >= n_rounds:\n",
    "            break\n",
    "            \n",
    "        context = contexts[log].reshape(10,10) # convert 1D flat context to (n_arms X ndims) 2D matrix\n",
    "        \n",
    "        # pull an arm with context provided if applicable\n",
    "        played_arm = mab.play(tround + 1, context)\n",
    "        \n",
    "        # if the arm played matches a given log\n",
    "        # note down the reward as if the bandit really received this reward\n",
    "        # and update the bandit with the played arm, reward, and context\n",
    "        # and increment supplied tround number only with the length of history recorded so far\n",
    "        if played_arm == arms[log]:\n",
    "            R.append(rewards[log])  \n",
    "            mab.update(played_arm, rewards[log], context)  \n",
    "            tround += 1 \n",
    "        \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset here\n",
    "dataset = np.loadtxt('dataset.txt')\n",
    "arms = dataset[:,0].astype(int)  # Column 1\n",
    "rewards = dataset[:,1].astype(int)  # Column 2\n",
    "contexts = dataset[:,2:].astype(int)  # Column 3:102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = EpsGreedy(10, 0.05)\n",
    "results_EpsGreedy = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('EpsGreedy average reward', np.mean(results_EpsGreedy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = UCB(10, 1.0)\n",
    "results_UCB = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('UCB average reward', np.mean(results_UCB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement LinUCB contextual MAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB(MAB):\n",
    "    \"\"\"Contextual multi-armed bandit (LinUCB)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    n_dims : int\n",
    "        Number of features for each arm's context.\n",
    "\n",
    "    alpha : float\n",
    "        Positive real explore-exploit parameter.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, n_dims, alpha):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "    \n",
    "    def play(self, context):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "    \n",
    "    def update(self, arm, reward, context):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = LinUCB(10, 10, 1.0)\n",
    "results_LinUCB = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('LinUCB average reward', np.mean(results_LinUCB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement TreeBootstrap contextual MAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeBootstrap(MAB):\n",
    "    \"\"\"Contextual Thompson sampled multi-armed bandit (TreeBootstrap)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    n_dims : int\n",
    "        Number of features for each arm's context.\n",
    "\n",
    "    tree : instance of sklearn.tree.DecisionTreeClassifier, optional\n",
    "        Decision tree to use for predicting the expected future reward. \n",
    "        Defaults to sklearn.tree.DecisionTreeClassifier().\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, n_dims, tree=DecisionTreeClassifier()):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "        \n",
    "    def play(self, context):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "    \n",
    "    def update(self, arm, reward, context):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = TreeBootstrap(10, 10)\n",
    "results_TreeBootstrap = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('TreeBootstrap average reward', np.mean(results_TreeBootstrap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and hyperparameter tuning for LinUCB\n",
    "### 5.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement KernelUCB contextual MAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelUCB(MAB):\n",
    "    \"\"\"Kernelised contextual multi-armed bandit (Kernelised LinUCB)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    n_dims : int\n",
    "        Number of features for each arm's context.\n",
    "\n",
    "    gamma : float\n",
    "        Positive real explore-exploit parameter.\n",
    "    \n",
    "    eta : float\n",
    "        Positive real explore-exploit parameter.\n",
    "    \n",
    "    kern : callable\n",
    "        A kernel function from sklearn.metrics.pairwise.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, n_dims, gamma, eta, kern):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "        \n",
    "    def play(self, context):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "    \n",
    "    def update(self, arm, reward, context):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = KernelUCB(10, 10, 1.0, 0.1, rbf_kernel)\n",
    "results_KernelUCB = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('KernelUCB average reward', np.mean(results_KernelUCB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your plotting code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}